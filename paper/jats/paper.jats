<?xml version="1.0" encoding="utf-8" ?>
<!DOCTYPE article PUBLIC "-//NLM//DTD JATS (Z39.96) Journal Publishing DTD v1.2 20190208//EN"
                  "JATS-publishing1.dtd">
<article xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:xlink="http://www.w3.org/1999/xlink" dtd-version="1.2" article-type="other">
<front>
<journal-meta>
<journal-id></journal-id>
<journal-title-group>
<journal-title>Journal of Open Source Software</journal-title>
<abbrev-journal-title>JOSS</abbrev-journal-title>
</journal-title-group>
<issn publication-format="electronic">2475-9066</issn>
<publisher>
<publisher-name>Open Journals</publisher-name>
</publisher>
</journal-meta>
<article-meta>
<article-id pub-id-type="publisher-id">0</article-id>
<article-id pub-id-type="doi">N/A</article-id>
<title-group>
<article-title>CogStim: Reproducible visual stimulus generation for
cognitive science, neuroscience, and vision research</article-title>
</title-group>
<contrib-group>
<contrib contrib-type="author">
<contrib-id contrib-id-type="orcid">https://orcid.org/0000-0001-8556-0469</contrib-id>
<name>
<surname>Correig-Fraga</surname>
<given-names>Eudald</given-names>
</name>
<xref ref-type="aff" rid="aff-1"/>
</contrib>
<aff id="aff-1">
<institution-wrap>
<institution>Innovamat Education, Sant Cugat del Vallès, Catalonia
(Spain)</institution>
</institution-wrap>
</aff>
</contrib-group>
<pub-date date-type="pub" publication-format="electronic" iso-8601-date="2025-11-08">
<day>8</day>
<month>11</month>
<year>2025</year>
</pub-date>
<volume>¿VOL?</volume>
<issue>¿ISSUE?</issue>
<fpage>¿PAGE?</fpage>
<permissions>
<copyright-statement>Authors of papers retain copyright and release the
work under a Creative Commons Attribution 4.0 International License (CC
BY 4.0)</copyright-statement>
<copyright-year>1970</copyright-year>
<copyright-holder>The article authors</copyright-holder>
<license license-type="open-access" xlink:href="https://creativecommons.org/licenses/by/4.0/">
<license-p>Authors of papers retain copyright and release the work under
a Creative Commons Attribution 4.0 International License (CC BY
4.0)</license-p>
</license>
</permissions>
<kwd-group kwd-group-type="author">
<kwd>Python</kwd>
<kwd>cognitive science</kwd>
<kwd>neuroscience</kwd>
<kwd>psychophysics</kwd>
<kwd>computer vision</kwd>
</kwd-group>
</article-meta>
</front>
<body>
<sec id="summary">
  <title>Summary</title>
  <p>CogStim is an open‑source Python library for reproducible,
  parameterized offline generation of visual stimuli for psychology,
  neuroscience, and computer vision. It produces PNG/JPEG/SVG assets for
  common paradigms (two‑colour ANS arrays, match‑to‑sample pairs with
  optional total‑area equalization, single‑colour dot arrays, geometric
  shapes, oriented lines/stripes, and fixation targets). Deterministic
  seeding and robust geometric routines enforce non‑overlap, boundary
  validity, and equalization to minimise perceptual confounds. A compact
  Python API and command‑line interface make the tool accessible to both
  programmers and non‑programmers, enabling quick creation of controlled
  stimuli without special setup.</p>
</sec>
<sec id="statement-of-need">
  <title>Statement of need</title>
  <p>Designing visual stimuli is a routine requirement in psychology,
  neuroscience, and vision research. It requires precise control over
  numerosity, size, spacing, color, and layout, alongside reproducible
  randomization and strict study dependent constraints. When built ad
  hoc, these demands make stimulus creation tedious and can introduce
  small inconsistencies that affect comparisons across studies and model
  evaluations. Researchers therefore benefit from a simple way to
  generate controlled stimuli offline as standard image assets ready to
  use wherever needed.</p>
  <p>CogStim addresses this need by providing an offline, parameterized
  generator for a range of paradigms: Approximate Number System (ANS)
  (<xref alt="Halberda et al., 2008" rid="ref-halberda_individual_2008" ref-type="bibr">Halberda
  et al., 2008</xref>), Match‑To‑Sample (MTS)
  (<xref alt="Sella et al., 2013" rid="ref-sella_enumeration_2013" ref-type="bibr">Sella
  et al., 2013</xref>), geometric shapes, oriented lines
  (<xref alt="Srinivasan, 2021" rid="ref-srinivasan_vision_2021" ref-type="bibr">Srinivasan,
  2021</xref>), and fixation targets
  (<xref alt="Thaler et al., 2013" rid="ref-thaler_what_2013" ref-type="bibr">Thaler
  et al., 2013</xref>). It produces images in any major image format
  (e.g. PNG, JPEG, SVG) that can be dropped into experiment builders,
  web or desktop presentation software, and computer‑vision pipelines
  without heavy setup. A Python API and a command‑line interface enable
  both quick prototyping and large batch generation; deterministic seeds
  ensure the same configuration yields identical outputs; and built‑in
  algorithms enforce non‑overlap and allow total‑area equalization when
  required. In this way the library serves dual needs: controlled
  stimuli for behavioral and neuro‑cognitive experiments, and
  well‑specified synthetic datasets for model development and
  evaluation.</p>
</sec>
<sec id="state-of-the-field">
  <title>State of the field</title>
  <p>Several established frameworks dominate the landscape of behavioral
  research: <monospace>PsychoPy</monospace>
  (<xref alt="Peirce et al., 2019" rid="ref-peirce_psychopy2_2019" ref-type="bibr">Peirce
  et al., 2019</xref>) is the standard Python library for experiment
  creation, offering precise timing and a vast array of stimuli;
  <monospace>Psychtoolbox</monospace>
  (<xref alt="Brainard, 1997" rid="ref-brainard_psychophysics_1997" ref-type="bibr">Brainard,
  1997</xref>) provides similar capabilities within the MATLAB/C
  environment with a focus on low-level hardware control; and
  <monospace>jsPsych</monospace>
  (<xref alt="Leeuw, 2015" rid="ref-de_leeuw_jspsych_2015" ref-type="bibr">Leeuw,
  2015</xref>) has become the de facto standard for web-based behavioral
  experiments. These tools are designed primarily as runtime engines:
  they generate and render stimuli in real-time during the experimental
  loop, coupling the stimulus generation logic with the display
  backend.</p>
  <p>CogStim was built rather than contributing to these existing
  projects for several strategic reasons. First, it decouples generation
  from presentation. While engines like PsychoPy are powerful,
  automating them to batch-export thousands of static images for
  external use (e.g., training a neural network or loading onto a
  tablet) often requires hacking the windowing system or running
  “headless” modes that are resource-intensive. CogStim is lightweight
  and backend-agnostic by design, treating file export as a first-class
  citizen. Second, CogStim fills a specific niche regarding algorithmic
  validity: it prioritizes the construction logic of the stimulus (e.g.,
  ensuring area equalization in ANS arrays or preventing overlap in
  crowded scenes) over the rendering speed required for runtime
  presentation. Finally, by producing standard assets (PNG/SVG) rather
  than experiment code, it ensures interoperability; the resulting
  datasets can be deployed across different platforms, from a custom web
  app to machine learning pipelines, making CogStim a more versatile
  tool for researchers.</p>
</sec>
<sec id="software-design">
  <title>Software design</title>
  <p>CogStim architecture prioritizes modularity and simplicity to
  remain accessible to researchers with varying programming expertise.
  We adopted a template method pattern where a base generator class
  handles infrastructure concerns—such as file I/O, directory structure,
  and random seed management—while specialized subclasses focus solely
  on the geometric logic of specific stimuli. This design decouples the
  “how” of file management from the “what” of stimulus creation,
  allowing researchers to extend the library with new generators without
  navigating complex boilerplate code.</p>
  <p>To ensure efficiency and maintainability, we minimized external
  dependencies, relying only on NumPy for vectorised geometric
  calculations and Pillow for rasterization. This lightweight footprint
  enables headless execution on servers or CI/CD pipelines, a trade-off
  we favoured over including heavy GUI dependencies often found in
  rendering engines.</p>
</sec>
<sec id="research-impact-statement">
  <title>Research impact statement</title>
  <p>CogStim has demonstrated its utility in diverse research contexts,
  bridging behavioral science and computational modeling. It has been
  used to generate stimuli for psychometric assessments in educational
  settings
  (<xref alt="Correig-Fraga et al., 2025" rid="ref-correig-fraga_interplay_2025" ref-type="bibr">Correig-Fraga
  et al., 2025</xref>;
  <xref alt="E. Correig-Fraga et al., 2024" rid="ref-correig-fraga_development_2024" ref-type="bibr">E.
  Correig-Fraga et al., 2024</xref>) and to create synthetic datasets
  for evaluating visual computation models in neuroscience (under
  review). These applications validate the library’s capability to
  support both traditional psychological experiments and modern
  data-driven approaches.</p>
  <p>To facilitate broad community adoption and ensure long-term
  reliability, CogStim adheres to rigorous software engineering
  standards. It includes a comprehensive test suite, continuous
  integration (CI) pipelines, and is distributed via PyPI under a
  permissive MIT license. Recognizing the varying technical expertise in
  the field, the project features a novel documentation strategy: an
  LLM-optimized manual designed to help non-programmers generate complex
  CLI commands via natural language prompting. This combination of
  robust engineering and accessibility effectively democratizes access
  to rigorous stimulus generation, ensuring that high-quality,
  reproducible stimuli are available to the wider research
  community.</p>
</sec>
<sec id="software-description">
  <title>Software description</title>
  <sec id="design-and-key-features">
    <title>Design and key features</title>
    <list list-type="bullet">
      <list-item>
        <p><bold>Task coverage</bold>: ANS two-colour dot arrays;
        single-colour dot arrays; MTS pairs with optional area
        equalization; geometric shapes (circle, star, triangle, square);
        oriented line/stripe patterns; fixation targets.</p>
      </list-item>
      <list-item>
        <p><bold>Determinism &amp; reproducibility</bold>: global seed
        handling for Python/NumPy; same parameters and same seed will
        yield identical images.</p>
      </list-item>
      <list-item>
        <p><bold>Robust dot engine</bold>:
        <monospace>DotsCore</monospace> enforces non-overlap, boundary
        validity, optional area equalization, and (for MTS) pair
        equalization within tolerances.</p>
      </list-item>
      <list-item>
        <p><bold>Stimulus equalization algorithms</bold>: CogStim
        implements robust geometric equalization methods that adjust dot
        radii so that total surface areas are matched either within
        two-colour ANS arrays or between sample–match pairs. These
        procedures guarantee perceptually fair stimuli for numerosity
        and matching tasks, maintaining non-overlap and boundary
        validity while achieving precise area ratios within configurable
        tolerances.</p>
      </list-item>
      <list-item>
        <p><bold>CLI &amp; Python API</bold>: consistent configuration
        via dictionaries in code and ergonomic subcommands in the
        CLI.</p>
      </list-item>
    </list>
  </sec>
  <sec id="implementation-and-dependencies">
    <title>Implementation and dependencies</title>
    <p>CogStim is implemented in Python (<inline-formula><alternatives>
    <tex-math><![CDATA[\ge]]></tex-math>
    <mml:math display="inline" xmlns:mml="http://www.w3.org/1998/Math/MathML"><mml:mo>≥</mml:mo></mml:math></alternatives></inline-formula>
    3.10)
    (<xref alt="Python Software Foundation, 2023" rid="ref-python" ref-type="bibr">Python
    Software Foundation, 2023</xref>) and builds upon a small number of
    widely used open-source libraries. Image creation and drawing
    operations are handled through Pillow
    (<xref alt="Clark, 2015" rid="ref-clark2015pillow" ref-type="bibr">Clark,
    2015</xref>), while all geometric computations and randomization
    routines rely on NumPy
    (<xref alt="Harris et al., 2020" rid="ref-harris2020array" ref-type="bibr">Harris
    et al., 2020</xref>). The library uses tqdm
    (<xref alt="Costa-Luis &amp; others, 2022" rid="ref-tqdm" ref-type="bibr">Costa-Luis
    &amp; others, 2022</xref>) to provide progress bars during
    generation processes and adopts standard Python modules such as
    argparse for command-line interfaces and pytest for automated
    testing.</p>
    <p>The codebase is organized around a small set of generator classes
    that call these dependencies through a unified interface. Each
    generator defines the parameters of a particular task (e.g., ANS,
    MTS, shapes, lines, fixation) and uses Pillow for rendering, NumPy
    for geometric calculations, and tqdm for user feedback. This results
    in a lightweight, portable implementation that can run on any system
    supporting Python without special dependencies or graphical
    backends.</p>
    <p>All dependencies are open source, actively maintained, and
    available through the Python Package Index (PyPI), ensuring
    long-term accessibility and compatibility with typical research
    workflows. The project is licensed under MIT, and available as a Git
    repository in Github.</p>
  </sec>
</sec>
<sec id="example-figures">
  <title>Example figures</title>
  <fig>
    <caption><p>Representative stimuli generated by
    CogStim.<styled-content id="figU003Astimuli"></styled-content></p></caption>
    <graphic mimetype="image" mime-subtype="png" xlink:href="stimuli_panel.png" />
  </fig>
  <p><bold>Figure 1.</bold> Representative stimuli generated by CogStim
  across different task paradigms. <bold>(a, b)</bold> Approximate
  Number System (ANS) two-colour dot arrays for numerosity
  discrimination tasks, with (b) showing area-equalized dots between
  colours. <bold>(c, d)</bold> Single geometric shapes (circle) in
  different colours, used in shape discrimination tasks. <bold>(e,
  f)</bold> Single-colour dot arrays suitable for numerosity estimation,
  match-to-sample (MTS) paradigms, or as components in multi-feature
  discrimination tasks. <bold>(g, h)</bold> Oriented line/stripe
  patterns for orientation discrimination experiments. <bold>(i, j,
  k)</bold> Additional geometric shapes (square, star, triangle) in
  various colours, demonstrating the library’s shape generation
  capabilities for categorical perception and visual search tasks.
  <bold>(l)</bold> Fixation cross stimulus for experimental trial
  preparation and gaze control.</p>
</sec>
<sec id="availability">
  <title>Availability</title>
  <list list-type="bullet">
    <list-item>
      <p>Repository: https://github.com/eudald-seeslab/cogstim</p>
    </list-item>
    <list-item>
      <p>License: MIT</p>
    </list-item>
    <list-item>
      <p>Issue tracker: enabled and publicly readable</p>
    </list-item>
    <list-item>
      <p>Archive: upon acceptance, we will create a tagged release,
      archive on Zenodo and include the DOI here.</p>
    </list-item>
  </list>
</sec>
<sec id="acknowledgements">
  <title>Acknowledgements</title>
  <p>We thank Innovamat Education for their support in the development
  of this open source work.</p>
</sec>
<sec id="ai-usage-disclosure">
  <title>AI usage disclosure</title>
  <p>The core architecture and foundational algorithms of CogStim were
  developed prior to the widespread adoption of generative AI tools.
  However, during recent development cycles, AI assistants were utilized
  to assist in refactoring legacy code, homogenizing interfaces across
  generator modules, and expanding specific functionalities to ensure
  consistency. Regarding this manuscript, generative AI tools were
  employed to review and refine the English language and narrative flow.
  The authors have manually verified all AI-generated suggestions and
  retain full responsibility for the accuracy and integrity of both the
  software and the publication.</p>
</sec>
<sec id="conflicts-of-interest">
  <title>Conflicts of Interest</title>
  <p>Authors declare no competing interests.</p>
</sec>
</body>
<back>
<ref-list>
  <title></title>
  <ref id="ref-halberda_individual_2008">
    <element-citation publication-type="article-journal">
      <person-group person-group-type="author">
        <name><surname>Halberda</surname><given-names>Justin</given-names></name>
        <name><surname>Mazzocco</surname><given-names>Michèle M. M.</given-names></name>
        <name><surname>Feigenson</surname><given-names>Lisa</given-names></name>
      </person-group>
      <article-title>Individual differences in non-verbal number acuity correlate with maths achievement</article-title>
      <source>Nature</source>
      <year iso-8601-date="2008-10-02">2008</year><month>10</month><day>02</day>
      <volume>455</volume>
      <issue>7213</issue>
      <pub-id pub-id-type="doi">10.1038/nature07246</pub-id>
      <pub-id pub-id-type="pmid">18776888</pub-id>
      <fpage>665</fpage>
      <lpage>668</lpage>
    </element-citation>
  </ref>
  <ref id="ref-thaler_what_2013">
    <element-citation publication-type="article-journal">
      <person-group person-group-type="author">
        <name><surname>Thaler</surname><given-names>L.</given-names></name>
        <name><surname>Schütz</surname><given-names>A. C.</given-names></name>
        <name><surname>Goodale</surname><given-names>M. A.</given-names></name>
        <name><surname>Gegenfurtner</surname><given-names>K. R.</given-names></name>
      </person-group>
      <article-title>What is the best fixation target? The effect of target shape on stability of fixational eye movements</article-title>
      <source>Vision Research</source>
      <year iso-8601-date="2013-01-14">2013</year><month>01</month><day>14</day>
      <date-in-citation content-type="access-date"><year iso-8601-date="2025-11-08">2025</year><month>11</month><day>08</day></date-in-citation>
      <volume>76</volume>
      <issn>0042-6989</issn>
      <uri>https://www.sciencedirect.com/science/article/pii/S0042698912003380</uri>
      <pub-id pub-id-type="doi">10.1016/j.visres.2012.10.012</pub-id>
      <fpage>31</fpage>
      <lpage>42</lpage>
    </element-citation>
  </ref>
  <ref id="ref-sella_enumeration_2013">
    <element-citation publication-type="article-journal">
      <person-group person-group-type="author">
        <name><surname>Sella</surname><given-names>Francesco</given-names></name>
        <name><surname>Lanfranchi</surname><given-names>Silvia</given-names></name>
        <name><surname>Zorzi</surname><given-names>Marco</given-names></name>
      </person-group>
      <article-title>Enumeration skills in down syndrome</article-title>
      <source>Research in Developmental Disabilities</source>
      <year iso-8601-date="2013-11">2013</year><month>11</month>
      <volume>34</volume>
      <issue>11</issue>
      <issn>1873-3379</issn>
      <pub-id pub-id-type="doi">10.1016/j.ridd.2013.07.038</pub-id>
      <pub-id pub-id-type="pmid">24025435</pub-id>
      <fpage>3798</fpage>
      <lpage>3806</lpage>
    </element-citation>
  </ref>
  <ref id="ref-srinivasan_vision_2021">
    <element-citation publication-type="article-journal">
      <person-group person-group-type="author">
        <name><surname>Srinivasan</surname><given-names>Mandyam V.</given-names></name>
      </person-group>
      <article-title>Vision, perception, navigation and ‘cognition’ in honeybees and applications to aerial robotics</article-title>
      <source>Biochemical and Biophysical Research Communications</source>
      <year iso-8601-date="2021-07-30">2021</year><month>07</month><day>30</day>
      <date-in-citation content-type="access-date"><year iso-8601-date="2024-12-05">2024</year><month>12</month><day>05</day></date-in-citation>
      <volume>564</volume>
      <issn>0006-291X</issn>
      <uri>https://www.sciencedirect.com/science/article/pii/S0006291X20317940</uri>
      <pub-id pub-id-type="doi">10.1016/j.bbrc.2020.09.052</pub-id>
      <fpage>4</fpage>
      <lpage>17</lpage>
    </element-citation>
  </ref>
  <ref id="ref-harris2020array">
    <element-citation publication-type="article-journal">
      <person-group person-group-type="author">
        <name><surname>Harris</surname><given-names>Charles R.</given-names></name>
        <name><surname>Millman</surname><given-names>K. Jarrod</given-names></name>
        <name><surname>Walt</surname><given-names>Stéfan J. van der</given-names></name>
        <name><surname>Gommers</surname><given-names>Ralf</given-names></name>
        <name><surname>Virtanen</surname><given-names>Pauli</given-names></name>
        <name><surname>Cournapeau</surname><given-names>David</given-names></name>
        <name><surname>Wieser</surname><given-names>Eric</given-names></name>
        <name><surname>Taylor</surname><given-names>Julian</given-names></name>
        <name><surname>Berg</surname><given-names>Sebastian</given-names></name>
        <name><surname>Smith</surname><given-names>Nathaniel J.</given-names></name>
        <name><surname>Kern</surname><given-names>Robert</given-names></name>
        <name><surname>Picus</surname><given-names>Matti</given-names></name>
        <name><surname>Hoyer</surname><given-names>Stephan</given-names></name>
        <name><surname>Kerkwijk</surname><given-names>Marten H. van</given-names></name>
        <name><surname>Brett</surname><given-names>Matthew</given-names></name>
        <name><surname>Haldane</surname><given-names>Allan</given-names></name>
        <name><surname>Río</surname><given-names>Jaime Fernández del</given-names></name>
        <name><surname>Wiebe</surname><given-names>Mark</given-names></name>
        <name><surname>Peterson</surname><given-names>Pearu</given-names></name>
        <name><surname>Gérard-Marchant</surname><given-names>Pierre</given-names></name>
        <name><surname>Sheppard</surname><given-names>Kevin</given-names></name>
        <name><surname>Reddy</surname><given-names>Tyler</given-names></name>
        <name><surname>Weckesser</surname><given-names>Warren</given-names></name>
        <name><surname>Abbasi</surname><given-names>Hameer</given-names></name>
        <name><surname>Gohlke</surname><given-names>Christoph</given-names></name>
        <name><surname>Oliphant</surname><given-names>Travis E.</given-names></name>
      </person-group>
      <article-title>Array programming with NumPy</article-title>
      <source>Nature</source>
      <publisher-name>Springer Science; Business Media LLC</publisher-name>
      <year iso-8601-date="2020-09">2020</year><month>09</month>
      <volume>585</volume>
      <issue>7825</issue>
      <uri>https://doi.org/10.1038/s41586-020-2649-2</uri>
      <pub-id pub-id-type="doi">10.1038/s41586-020-2649-2</pub-id>
      <fpage>357</fpage>
      <lpage>362</lpage>
    </element-citation>
  </ref>
  <ref id="ref-clark2015pillow">
    <element-citation>
      <person-group person-group-type="author">
        <name><surname>Clark</surname><given-names>Alex</given-names></name>
      </person-group>
      <article-title>Pillow (PIL fork) documentation</article-title>
      <publisher-name>readthedocs</publisher-name>
      <year iso-8601-date="2015">2015</year>
      <uri>https://buildmedia.readthedocs.org/media/pdf/pillow/latest/pillow.pdf</uri>
    </element-citation>
  </ref>
  <ref id="ref-correig-fraga_development_2024">
    <element-citation publication-type="article-journal">
      <person-group person-group-type="author">
        <name><surname>Correig-Fraga</surname><given-names>Eudald</given-names></name>
        <name><surname>Vilalta-Riera</surname><given-names>Albert</given-names></name>
        <name><surname>Calvo-Pesce</surname><given-names>Cecilia</given-names></name>
      </person-group>
      <article-title>Development and validation of a semi-automated, scalable response to intervention framework in mathematics</article-title>
      <source>SN Social Sciences</source>
      <year iso-8601-date="2024-02-01">2024</year><month>02</month><day>01</day>
      <date-in-citation content-type="access-date"><year iso-8601-date="2025-02-19">2025</year><month>02</month><day>19</day></date-in-citation>
      <volume>4</volume>
      <issue>2</issue>
      <issn>2662-9283</issn>
      <uri>https://link.springer.com/article/10.1007/s43545-024-00835-7</uri>
      <pub-id pub-id-type="doi">10.1007/s43545-024-00835-7</pub-id>
      <fpage>1</fpage>
      <lpage>19</lpage>
    </element-citation>
  </ref>
  <ref id="ref-correig-fraga_interplay_2025">
    <element-citation publication-type="article-journal">
      <person-group person-group-type="author">
        <name><surname>Correig-Fraga</surname></name>
        <name><surname>Sales-Pardo</surname></name>
        <name><surname>Guimerà</surname><given-names>Roger</given-names></name>
      </person-group>
      <article-title>Interplay between children’s cognitive profiles and within-school social interactions is nuanced and differs across ages</article-title>
      <source>Communications psychology</source>
      <year iso-8601-date="2025-03-20">2025</year><month>03</month><day>20</day>
      <date-in-citation content-type="access-date"><year iso-8601-date="2025-11-08">2025</year><month>11</month><day>08</day></date-in-citation>
      <volume>3</volume>
      <issue>1</issue>
      <issn>2731-9121</issn>
      <uri>https://pubmed.ncbi.nlm.nih.gov/40113993/</uri>
      <pub-id pub-id-type="doi">10.1038/s44271-025-00227-4</pub-id>
      <pub-id pub-id-type="pmid">40113993</pub-id>
    </element-citation>
  </ref>
  <ref id="ref-tqdm">
    <element-citation>
      <person-group person-group-type="author">
        <name><surname>Costa-Luis</surname><given-names>Carlos da</given-names></name>
        <name><surname>others</surname></name>
      </person-group>
      <article-title>Tqdm: A fast, extensible progress bar for python and CLI</article-title>
      <publisher-name>https://github.com/tqdm/tqdm</publisher-name>
      <year iso-8601-date="2022">2022</year>
    </element-citation>
  </ref>
  <ref id="ref-python">
    <element-citation>
      <person-group person-group-type="author">
        <string-name>Python Software Foundation</string-name>
      </person-group>
      <article-title>Python: A programming language for scientific computing</article-title>
      <publisher-name>https://www.python.org/</publisher-name>
      <year iso-8601-date="2023">2023</year>
    </element-citation>
  </ref>
  <ref id="ref-peirce_psychopy2_2019">
    <element-citation publication-type="article-journal">
      <person-group person-group-type="author">
        <name><surname>Peirce</surname><given-names>Jonathan</given-names></name>
        <name><surname>Gray</surname><given-names>Jeremy R.</given-names></name>
        <name><surname>Simpson</surname><given-names>Sol</given-names></name>
        <name><surname>MacAskill</surname><given-names>Michael</given-names></name>
        <name><surname>Höchenberger</surname><given-names>Richard</given-names></name>
        <name><surname>Sogo</surname><given-names>Hiroyuki</given-names></name>
        <name><surname>Kastman</surname><given-names>Erik</given-names></name>
        <name><surname>Lindeløv</surname><given-names>Jonas Kristoffer</given-names></name>
      </person-group>
      <article-title>PsychoPy2: Experiments in behavior made easy</article-title>
      <source>Behavior Research Methods</source>
      <year iso-8601-date="2019-02-01">2019</year><month>02</month><day>01</day>
      <date-in-citation content-type="access-date"><year iso-8601-date="2026-02-02">2026</year><month>02</month><day>02</day></date-in-citation>
      <volume>51</volume>
      <issue>1</issue>
      <issn>1554-3528</issn>
      <uri>https://doi.org/10.3758/s13428-018-01193-y</uri>
      <pub-id pub-id-type="doi">10.3758/s13428-018-01193-y</pub-id>
      <fpage>195</fpage>
      <lpage>203</lpage>
    </element-citation>
  </ref>
  <ref id="ref-brainard_psychophysics_1997">
    <element-citation publication-type="article-journal">
      <person-group person-group-type="author">
        <name><surname>Brainard</surname><given-names>D. H.</given-names></name>
      </person-group>
      <article-title>The psychophysics toolbox</article-title>
      <source>Spatial Vision</source>
      <year iso-8601-date="1997">1997</year>
      <volume>10</volume>
      <issue>4</issue>
      <issn>0169-1015</issn>
      <fpage>433</fpage>
      <lpage>436</lpage>
    </element-citation>
  </ref>
  <ref id="ref-de_leeuw_jspsych_2015">
    <element-citation publication-type="article-journal">
      <person-group person-group-type="author">
        <name><surname>Leeuw</surname><given-names>Joshua R. de</given-names></name>
      </person-group>
      <article-title>jsPsych: A JavaScript library for creating behavioral experiments in a web browser</article-title>
      <source>Behavior Research Methods</source>
      <year iso-8601-date="2015-03-01">2015</year><month>03</month><day>01</day>
      <date-in-citation content-type="access-date"><year iso-8601-date="2026-02-02">2026</year><month>02</month><day>02</day></date-in-citation>
      <volume>47</volume>
      <issue>1</issue>
      <issn>1554-3528</issn>
      <uri>https://doi.org/10.3758/s13428-014-0458-y</uri>
      <pub-id pub-id-type="doi">10.3758/s13428-014-0458-y</pub-id>
      <fpage>1</fpage>
      <lpage>12</lpage>
    </element-citation>
  </ref>
</ref-list>
</back>
</article>
